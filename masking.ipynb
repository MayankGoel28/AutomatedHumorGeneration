{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62744c2b",
   "metadata": {
    "id": "62744c2b"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yHma1sN-q8gl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "854bd183f6f2494494c670d071aeb959",
      "b3e4c8f3e53c4eb98a800f3c36e78c0c",
      "f4d2d10d516845d5bc63e23cd0d6f099",
      "d9a318b1a95d4e86b9272c0b2bdcebbc",
      "6944befc8fd34698b85f6c773a2c55a1",
      "aed91f6e635b4c29a6099b9914741897",
      "81a49e890316453f8b72743e38be4129",
      "28cc239a30b7494aa2b7fb667527caab",
      "3af998f994924adcb46664dd35b58ce8",
      "c29bb58d4aba44bba7e76089d7fee812",
      "77c13019fe4d41d4a9a452dd0bf62ab5"
     ]
    },
    "id": "yHma1sN-q8gl",
    "outputId": "53a8d506-9edb-4321-a1eb-b03ae2ae3f3f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../ilm/new_final_model were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../ilm/new_final_model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ../ilm/new_final_model were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../ilm/new_final_model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Non-consecutive added token '[PAD]' found. Should have index 30522 but has index 0 in saved vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load BERT tokenizer and model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#tokenizer_w = BertTokenizer.from_pretrained(model_name)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model_w \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m word_embedding_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTransformer(model_name)\n\u001b[1;32m      8\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mPooling(word_embedding_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model_s \u001b[38;5;241m=\u001b[39m SentenceTransformer(modules\u001b[38;5;241m=\u001b[39m[word_embedding_model, pooling_model])\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:31\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:693\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1812\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1810\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   1813\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1814\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1815\u001b[0m     init_configuration,\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   1817\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1818\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1819\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1820\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   1821\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1823\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1844\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m has_tokenizer_file \u001b[38;5;241m=\u001b[39m resolved_vocab_files\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1844\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class)\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   1845\u001b[0m         copy\u001b[38;5;241m.\u001b[39mdeepcopy(resolved_vocab_files),\n\u001b[1;32m   1846\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   1847\u001b[0m         copy\u001b[38;5;241m.\u001b[39mdeepcopy(init_configuration),\n\u001b[1;32m   1848\u001b[0m         \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   1849\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[1;32m   1850\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1851\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1852\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[1;32m   1853\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)),\n\u001b[1;32m   1854\u001b[0m     )\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2031\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2025\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong index found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2026\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2027\u001b[0m     )\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m current_index:\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;66;03m# Tokenizer slow: added token cannot already be in the vocabulary so its index needs to be the\u001b[39;00m\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;66;03m# current length of the tokenizer.\u001b[39;00m\n\u001b[0;32m-> 2031\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-consecutive added token \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2033\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in saved vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2034\u001b[0m     )\n\u001b[1;32m   2036\u001b[0m is_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(token \u001b[38;5;129;01min\u001b[39;00m special_tokens)\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_last_special \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m is_last_special \u001b[38;5;241m==\u001b[39m is_special:\n",
      "\u001b[0;31mValueError\u001b[0m: Non-consecutive added token '[PAD]' found. Should have index 30522 but has index 0 in saved vocabulary."
     ]
    }
   ],
   "source": [
    "# Specify the BERT model variant you want to use\n",
    "model_name = \"../ilm/new_final_model\"\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "#tokenizer_w = BertTokenizer.from_pretrained(model_name)\n",
    "model_w = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
    "model_s = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1583f5",
   "metadata": {
    "id": "7f1583f5"
   },
   "outputs": [],
   "source": [
    "def get_attention_matrix(sentence, target_words, tokenizer, model):\n",
    "    # Initialize tokenizer and model\n",
    "\n",
    "    # Tokenize input and obtain outputs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get attention weights from outputs\n",
    "    attentions = outputs.attentions  # List of attention tensors for each layer\n",
    "\n",
    "    # Define weights for the last four layers\n",
    "    weights = torch.tensor([0.1, 0.2, 0.3, 0.4], device=attentions[0].device)\n",
    "\n",
    "    # Extract the last four layers and apply weights\n",
    "    last_four_layers = torch.stack(attentions[-4:])\n",
    "    weighted_layers = last_four_layers * weights[:, None, None, None, None]\n",
    "\n",
    "    # Sum across the weighted layers and then average over the heads\n",
    "    weighted_sum = torch.sum(weighted_layers, dim=0)\n",
    "    avg_attention = torch.mean(weighted_sum, dim=1)[0]\n",
    "\n",
    "    # Aggregate subword attentions for whole words\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_attention_list = []\n",
    "    word_list = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if not tokens[i].startswith(\"##\"):\n",
    "            word = tokens[i]\n",
    "            if i == len(tokens) - 1 or not tokens[i+1].startswith(\"##\"):\n",
    "                word_attention_list.append(avg_attention[i].tolist())\n",
    "            else:\n",
    "                subword_count = 1\n",
    "                subword_attention = avg_attention[i].clone()\n",
    "                word += tokens[i+1][2:]\n",
    "                while i + subword_count < len(tokens) and tokens[i + subword_count].startswith(\"##\"):\n",
    "                    subword_attention += avg_attention[i + subword_count]\n",
    "                    word += tokens[i + subword_count + 1][2:] if i + subword_count + 1 < len(tokens) and tokens[i + subword_count + 1].startswith(\"##\") else \"\"\n",
    "                    subword_count += 1\n",
    "                word_attention_list.append((subword_attention / subword_count).tolist())\n",
    "                i += subword_count - 1\n",
    "            word_list.append(word)\n",
    "        i += 1\n",
    "\n",
    "   # Convert attention to dictionary form for whole words\n",
    "    attention_dict = {}\n",
    "    for i, word in enumerate(word_list):\n",
    "        attention_dict[word] = {word_list[j]: word_attention_list[i][j] for j in range(len(word_list))}\n",
    "\n",
    "    # Compute importance scores for all words\n",
    "    all_importance_scores = {}\n",
    "    for word, weights in attention_dict.items():\n",
    "        all_importance_scores[word] = sum(weights.values())\n",
    "\n",
    "    # Extract importance scores for target words\n",
    "    importance_scores = {word: all_importance_scores[word] for word in target_words if word in all_importance_scores}\n",
    "\n",
    "    # Normalize the importance scores so they sum to 1 for the target words\n",
    "    total_score = sum(importance_scores.values())\n",
    "    for word in importance_scores:\n",
    "        importance_scores[word] /= total_score\n",
    "\n",
    "    return attention_dict, importance_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c7de1",
   "metadata": {
    "id": "bd2c7de1"
   },
   "outputs": [],
   "source": [
    "def get_semantic_weights(sentence, matcher):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for match in matches:\n",
    "        match_id, start, end = match\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        # print(span.text)\n",
    "        chunks.append((start, end))\n",
    "\n",
    "    chunk_phrases = [str(doc[start:end]) for start, end in chunks]\n",
    "    attention_dict, importance_scores = get_attention_matrix(sentence, chunk_phrases, tokenizer_w, model_w)\n",
    "    s_embedding = model_s.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    weights = []\n",
    "    for chunk in chunks:\n",
    "\n",
    "        start, end = chunk\n",
    "        chunk_phrase = doc[start:end]\n",
    "\n",
    "        new_sent = ' '.join([doc[:start].text, doc[end:].text])\n",
    "        # sentence.replace(chunk, '')\n",
    "        new_embedding = model_s.encode(new_sent, convert_to_tensor=True)\n",
    "        # print(chunk_phrase, \": \", new_sent)\n",
    "        cosine_score = util.cos_sim(s_embedding, new_embedding)\n",
    "        weights.append(((start, end), 1-cosine_score.cpu().squeeze().numpy()))\n",
    "\n",
    "    total = sum([score for (chunk, score) in weights])\n",
    "    weights = [(chunk, score/total) for (chunk, score) in weights]\n",
    "\n",
    "    weights.sort(key = lambda x : x[1], reverse = True)\n",
    "    sem_weight_scores = {}\n",
    "    for (start, end), weight in weights:\n",
    "      sem_weight_scores[str(doc[start:end])] = weight\n",
    "    return sem_weight_scores, importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926aa2fb",
   "metadata": {
    "id": "926aa2fb"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "pattern = [{\"POS\": {\"IN\": [\"NOUN\", \"PROPN\",\"VERB\",\"ADJ\",\"ADV\"]}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"pattern\",[pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yBr2-lMakUjh",
   "metadata": {
    "id": "yBr2-lMakUjh"
   },
   "outputs": [],
   "source": [
    "sample = \"why is the fish fishing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l4oH9DWbkKYQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4oH9DWbkKYQ",
    "outputId": "3e37dca2-fdd1-47e6-8ed8-aa541d1ab615"
   },
   "outputs": [],
   "source": [
    "weights, scores = get_semantic_weights(sample, matcher)\n",
    "print(weights, scores)\n",
    "# for (start, end), weight in weights:\n",
    "#     print((start,end), doc[start:end], \":\", weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yXjW_7LHubpW",
   "metadata": {
    "id": "yXjW_7LHubpW"
   },
   "outputs": [],
   "source": [
    "def mask_function(document, words_to_mask):\n",
    "    \"\"\"\n",
    "    Returns a list of 3-tuples indicating positions of masked words.\n",
    "\n",
    "    Parameters:\n",
    "    - document (str): The input document.\n",
    "    - words_to_mask (list): List of words that need to be masked.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of 3-tuples (infilling type, span offset, span length).\n",
    "    \"\"\"\n",
    "\n",
    "    masked_positions = []\n",
    "\n",
    "    for word in words_to_mask:\n",
    "        offset = document.find(word)\n",
    "        while offset != -1:\n",
    "            masked_positions.append((\"mask\", offset, len(word)))\n",
    "            offset = document.find(word, offset + len(word))\n",
    "\n",
    "    # Sort by offsets to ensure the list is in order\n",
    "    return sorted(masked_positions, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f606139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "def get_masked_template(sentence, n=0.5, semantic_score_weight=0.5):\n",
    "    # 1. Split the joke into setup and punchline.\n",
    "    setup, punchline = sentence.split(\"?\")\n",
    "    \n",
    "    # 2. Calculate semantic and attention scores for the entire sentence.\n",
    "    sem_weight_scores, attention_scores = get_semantic_weights(sentence, matcher)\n",
    "    \n",
    "    # Create a final_score dictionary just as before.\n",
    "    final_score = {}\n",
    "    for k in sem_weight_scores:\n",
    "        final_score[k] = semantic_score_weight*sem_weight_scores[k] + (1-semantic_score_weight)*attention_scores[k]\n",
    "    \n",
    "    # 3. Determine words to be masked for setup and punchline.\n",
    "    # Using regular expression to split the text by spaces and punctuations.\n",
    "    setup_words = re.findall(r'\\b\\w+\\b', setup)\n",
    "    punchline_words = re.findall(r'\\b\\w+\\b', punchline)\n",
    "    \n",
    "    setup_scores = {word: final_score[word] for word in setup_words if word in final_score}\n",
    "    punchline_scores = {word: final_score[word] for word in punchline_words if word in final_score}\n",
    "\n",
    "    setup_masked_words = [k for k, v in sorted(setup_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    punchline_masked_words = [k for k, v in sorted(punchline_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    final_setup_masked_words = setup_masked_words[:math.ceil(len(setup_masked_words)*n)]\n",
    "    final_punchline_masked_words = punchline_masked_words[:math.ceil(len(punchline_masked_words)*n)]\n",
    "    \n",
    "    # 4. Mask the setup and punchline separately.\n",
    "    masked_setup = mask_function(setup, final_setup_masked_words)\n",
    "    masked_punchline = mask_function(punchline, final_punchline_masked_words)\n",
    "    \n",
    "    # 5. Combine the masked setup and punchline.\n",
    "    masked_joke = masked_setup + \"?\" + masked_punchline\n",
    "\n",
    "    return final_score, masked_joke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i7he2BkmkW5V",
   "metadata": {
    "id": "i7he2BkmkW5V"
   },
   "outputs": [],
   "source": [
    "# def get_masked_template(sentence, n=0.5, semantic_score_weight=0.5):\n",
    "#   sem_weight_scores, attention_scores = get_semantic_weights(sentence, matcher)\n",
    "#   final_score = {}\n",
    "#   for k in sem_weight_scores:\n",
    "#     final_score[k] = semantic_score_weight*sem_weight_scores[k] + (1-semantic_score_weight)*attention_scores[k]\n",
    "#   masked_words = [k for k, v in sorted(final_score.items(), key=lambda item: item[1], reverse=True)]\n",
    "#   final_masked_words = masked_words[:math.ceil(len(masked_words)*n)]\n",
    "#   return final_score, mask_function(sentence, final_masked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vsqfl78WtTzY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsqfl78WtTzY",
    "outputId": "820022cb-e939-46e5-f4ed-6824e55d43be"
   },
   "outputs": [],
   "source": [
    "get_masked_template(\"i am fishing for fishies because i love food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUKcRjsZul77",
   "metadata": {
    "id": "IUKcRjsZul77"
   },
   "outputs": [],
   "source": [
    "def apply_mask(sentence, mask_spans):\n",
    "    \"\"\"\n",
    "    Masks the specified spans in the sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The input sentence.\n",
    "    - mask_spans (list): List of 3-tuples specifying spans to mask.\n",
    "\n",
    "    Returns:\n",
    "    - str: Sentence with specified spans replaced by [MASK].\n",
    "    \"\"\"\n",
    "\n",
    "    # Reverse the list so that we can mask from the end of the sentence.\n",
    "    # This ensures that the earlier offsets don't change.\n",
    "    mask_spans.reverse()\n",
    "\n",
    "    for _, offset, length in mask_spans:\n",
    "        sentence = sentence[:offset] + '[MASK]' + sentence[offset + length:]\n",
    "\n",
    "    return sentence\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28cc239a30b7494aa2b7fb667527caab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3af998f994924adcb46664dd35b58ce8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6944befc8fd34698b85f6c773a2c55a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77c13019fe4d41d4a9a452dd0bf62ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81a49e890316453f8b72743e38be4129": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "854bd183f6f2494494c670d071aeb959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3e4c8f3e53c4eb98a800f3c36e78c0c",
       "IPY_MODEL_f4d2d10d516845d5bc63e23cd0d6f099",
       "IPY_MODEL_d9a318b1a95d4e86b9272c0b2bdcebbc"
      ],
      "layout": "IPY_MODEL_6944befc8fd34698b85f6c773a2c55a1"
     }
    },
    "aed91f6e635b4c29a6099b9914741897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e4c8f3e53c4eb98a800f3c36e78c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aed91f6e635b4c29a6099b9914741897",
      "placeholder": "​",
      "style": "IPY_MODEL_81a49e890316453f8b72743e38be4129",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "c29bb58d4aba44bba7e76089d7fee812": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9a318b1a95d4e86b9272c0b2bdcebbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c29bb58d4aba44bba7e76089d7fee812",
      "placeholder": "​",
      "style": "IPY_MODEL_77c13019fe4d41d4a9a452dd0bf62ab5",
      "value": " 466k/466k [00:00&lt;00:00, 840kB/s]"
     }
    },
    "f4d2d10d516845d5bc63e23cd0d6f099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28cc239a30b7494aa2b7fb667527caab",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3af998f994924adcb46664dd35b58ce8",
      "value": 466062
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
