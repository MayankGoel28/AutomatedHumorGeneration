{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62744c2b",
   "metadata": {
    "id": "62744c2b"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "import spacy_transformers\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2c16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yHma1sN-q8gl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "854bd183f6f2494494c670d071aeb959",
      "b3e4c8f3e53c4eb98a800f3c36e78c0c",
      "f4d2d10d516845d5bc63e23cd0d6f099",
      "d9a318b1a95d4e86b9272c0b2bdcebbc",
      "6944befc8fd34698b85f6c773a2c55a1",
      "aed91f6e635b4c29a6099b9914741897",
      "81a49e890316453f8b72743e38be4129",
      "28cc239a30b7494aa2b7fb667527caab",
      "3af998f994924adcb46664dd35b58ce8",
      "c29bb58d4aba44bba7e76089d7fee812",
      "77c13019fe4d41d4a9a452dd0bf62ab5"
     ]
    },
    "id": "yHma1sN-q8gl",
    "outputId": "53a8d506-9edb-4321-a1eb-b03ae2ae3f3f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert_model/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert_model/ and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Specify the BERT model variant you want to use\n",
    "model_name = \"bert_model/\"\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer_w = BertTokenizer.from_pretrained(model_name)\n",
    "model_w = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
    "model_s = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1583f5",
   "metadata": {
    "id": "7f1583f5"
   },
   "outputs": [],
   "source": [
    "def get_attention_matrix(sentence, target_words, tokenizer, model):\n",
    "    # Initialize tokenizer and model\n",
    "\n",
    "    # Tokenize input and obtain outputs\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get attention weights from outputs\n",
    "    attentions = outputs.attentions  # List of attention tensors for each layer\n",
    "\n",
    "    # Define weights for the last four layers\n",
    "    weights = torch.tensor([0.1, 0.2, 0.3, 0.4], device=attentions[0].device)\n",
    "\n",
    "    # Extract the last four layers and apply weights\n",
    "    last_four_layers = torch.stack(attentions[-4:])\n",
    "    weighted_layers = last_four_layers * weights[:, None, None, None, None]\n",
    "\n",
    "    # Sum across the weighted layers and then average over the heads\n",
    "    weighted_sum = torch.sum(weighted_layers, dim=0)\n",
    "    avg_attention = torch.mean(weighted_sum, dim=1)[0]\n",
    "\n",
    "    # Aggregate subword attentions for whole words\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_attention_list = []\n",
    "    word_list = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if not tokens[i].startswith(\"##\"):\n",
    "            word = tokens[i]\n",
    "            if i == len(tokens) - 1 or not tokens[i+1].startswith(\"##\"):\n",
    "                word_attention_list.append(avg_attention[i].tolist())\n",
    "            else:\n",
    "                subword_count = 1\n",
    "                subword_attention = avg_attention[i].clone()\n",
    "                word += tokens[i+1][2:]\n",
    "                while i + subword_count < len(tokens) and tokens[i + subword_count].startswith(\"##\"):\n",
    "                    subword_attention += avg_attention[i + subword_count]\n",
    "                    word += tokens[i + subword_count + 1][2:] if i + subword_count + 1 < len(tokens) and tokens[i + subword_count + 1].startswith(\"##\") else \"\"\n",
    "                    subword_count += 1\n",
    "                word_attention_list.append((subword_attention / subword_count).tolist())\n",
    "                i += subword_count - 1\n",
    "            word_list.append(word)\n",
    "        i += 1\n",
    "\n",
    "   # Convert attention to dictionary form for whole words\n",
    "    attention_dict = {}\n",
    "    for i, word in enumerate(word_list):\n",
    "        attention_dict[word] = {word_list[j]: word_attention_list[i][j] for j in range(len(word_list))}\n",
    "\n",
    "    # Compute importance scores for all words\n",
    "    all_importance_scores = {}\n",
    "    for word, weights in attention_dict.items():\n",
    "        all_importance_scores[word] = sum(weights.values())\n",
    "\n",
    "    # Extract importance scores for target words\n",
    "    importance_scores = {word: all_importance_scores[word] for word in target_words if word in all_importance_scores}\n",
    "\n",
    "    # Normalize the importance scores so they sum to 1 for the target words\n",
    "    total_score = sum(importance_scores.values())\n",
    "    for word in importance_scores:\n",
    "        importance_scores[word] /= total_score\n",
    "\n",
    "    return attention_dict, importance_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e548bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency_importance(joke, target_words):\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462f683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_weights(sentence, matcher):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for match in matches:\n",
    "        match_id, start, end = match\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        # print(span.text)\n",
    "        chunks.append((start, end))\n",
    "\n",
    "    chunk_phrases = [str(doc[start:end]) for start, end in chunks]\n",
    "    s_embedding = model_s.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    weights = []\n",
    "    for chunk in chunks:\n",
    "\n",
    "        start, end = chunk\n",
    "        chunk_phrase = doc[start:end]\n",
    "\n",
    "        new_sent = ' '.join([doc[:start].text, doc[end:].text])\n",
    "        # sentence.replace(chunk, '')\n",
    "        new_embedding = model_s.encode(new_sent, convert_to_tensor=True)\n",
    "        # print(chunk_phrase, \": \", new_sent)\n",
    "        cosine_score = util.cos_sim(s_embedding, new_embedding)\n",
    "        weights.append(((start, end), 1-cosine_score.cpu().squeeze().numpy()))\n",
    "\n",
    "    total = sum([score for (chunk, score) in weights])\n",
    "    weights = [(chunk, score/total) for (chunk, score) in weights]\n",
    "\n",
    "    weights.sort(key = lambda x : x[1], reverse = True)\n",
    "    sem_weight_scores = {}\n",
    "    for (start, end), weight in weights:\n",
    "      sem_weight_scores[str(doc[start:end])] = weight\n",
    "    return sem_weight_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2c7de1",
   "metadata": {
    "id": "bd2c7de1"
   },
   "outputs": [],
   "source": [
    "def get_semantic_weights(sentence, matcher):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for match in matches:\n",
    "        match_id, start, end = match\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        # print(span.text)\n",
    "        chunks.append((start, end))\n",
    "\n",
    "    chunk_phrases = [str(doc[start:end]) for start, end in chunks]\n",
    "    attention_dict, importance_scores = get_attention_matrix(sentence, chunk_phrases, tokenizer_w, model_w)\n",
    "    dependency_scores = dependency_importance(sentence, chunk_phrases)\n",
    "    s_embedding = model_s.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "    weights = []\n",
    "    for chunk in chunks:\n",
    "\n",
    "        start, end = chunk\n",
    "        chunk_phrase = doc[start:end]\n",
    "\n",
    "        new_sent = ' '.join([doc[:start].text, doc[end:].text])\n",
    "        # sentence.replace(chunk, '')\n",
    "        new_embedding = model_s.encode(new_sent, convert_to_tensor=True)\n",
    "        # print(chunk_phrase, \": \", new_sent)\n",
    "        cosine_score = util.cos_sim(s_embedding, new_embedding)\n",
    "        weights.append(((start, end), 1-cosine_score.cpu().squeeze().numpy()))\n",
    "\n",
    "    total = sum([score for (chunk, score) in weights])\n",
    "    weights = [(chunk, score/total) for (chunk, score) in weights]\n",
    "\n",
    "    weights.sort(key = lambda x : x[1], reverse = True)\n",
    "    sem_weight_scores = {}\n",
    "    for (start, end), weight in weights:\n",
    "      sem_weight_scores[str(doc[start:end])] = weight\n",
    "    return sem_weight_scores, importance_scores, dependency_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "926aa2fb",
   "metadata": {
    "id": "926aa2fb"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "pattern = [{\"POS\": {\"IN\": [\"NOUN\", \"PROPN\",\"ADJ\",\"ADV\"]}}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"pattern\",[pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "yBr2-lMakUjh",
   "metadata": {
    "id": "yBr2-lMakUjh"
   },
   "outputs": [],
   "source": [
    "sample = \"why is the fish fishing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yXjW_7LHubpW",
   "metadata": {
    "id": "yXjW_7LHubpW"
   },
   "outputs": [],
   "source": [
    "def mask_function(document, words_to_mask):\n",
    "    \"\"\"\n",
    "    Returns a list of 3-tuples indicating positions of masked words.\n",
    "\n",
    "    Parameters:\n",
    "    - document (str): The input document.\n",
    "    - words_to_mask (list): List of words that need to be masked.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of 3-tuples (infilling type, span offset, span length).\n",
    "    \"\"\"\n",
    "\n",
    "    masked_positions = []\n",
    "\n",
    "    for word in words_to_mask:\n",
    "        offset = document.find(word)\n",
    "        while offset != -1:\n",
    "            masked_positions.append((\"mask\", offset, len(word)))\n",
    "            offset = document.find(word, offset + len(word))\n",
    "\n",
    "    # Sort by offsets to ensure the list is in order\n",
    "    return sorted(masked_positions, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b3b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... [rest of your code above]\n",
    "# w1 - semantic, w2 - attention, w3 - dependency\n",
    "def get_masked_template(sentence, n=0.5, w1=0.5, w2=0.5, w3=0):\n",
    "    if sentence.count('?'):\n",
    "        # 1. Split the joke into setup and punchline.\n",
    "        setup, punchline = sentence.rsplit(\"?\", 1)\n",
    "\n",
    "        # 2. Calculate semantic and attention scores for the entire sentence.\n",
    "        sem_weight_scores, attention_scores, dependency_scores = get_semantic_weights(sentence, matcher)\n",
    "\n",
    "        # Create a final_score dictionary just as before.\n",
    "        final_score = {}\n",
    "        for k in sem_weight_scores:\n",
    "            try:\n",
    "                final_score[k] = w1*sem_weight_scores[k] + w2*attention_scores[k] + w3*dependency_scores[k]\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                final_score[k] = sem_weight_scores[k]\n",
    "        # 3. Determine words to be masked for setup and punchline.\n",
    "        setup_words = re.findall(r'\\b\\w+\\b', setup)\n",
    "        punchline_words = re.findall(r'\\b\\w+\\b', punchline)\n",
    "\n",
    "        setup_scores = {word: final_score[word] for word in setup_words if word in final_score}\n",
    "        punchline_scores = {word: final_score[word] for word in punchline_words if word in final_score}\n",
    "\n",
    "        setup_masked_words = [k for k, v in sorted(setup_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "        punchline_masked_words = [k for k, v in sorted(punchline_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "        total_candidates = setup_masked_words + punchline_masked_words\n",
    "        #print(setup_masked_words, punchline_masked_words)\n",
    "        \n",
    "        final_setup_masked_words = setup_masked_words[:math.ceil(len(setup_masked_words)*n)]\n",
    "        final_punchline_masked_words = punchline_masked_words[:math.ceil(len(punchline_masked_words)*n)]\n",
    "        #print(final_punchline_masked_words, final_setup_masked_words)\n",
    "\n",
    "        # 4. Mask the setup and punchline separately.\n",
    "        masked_setup = mask_function(setup, final_setup_masked_words)\n",
    "        masked_punchline = mask_function(punchline, final_punchline_masked_words)\n",
    "        adjustment = len(setup) + 1  # +1 to account for the question mark\n",
    "        masked_punchline = [(typ, offset + adjustment, length) for typ, offset, length in masked_punchline]\n",
    "\n",
    "        # 5. Combine the masked setup and punchline.\n",
    "        masked_joke_spans = masked_setup + masked_punchline\n",
    "\n",
    "        return sorted(masked_joke_spans, key=lambda x: x[1]), total_candidates\n",
    "    else:\n",
    "        sem_weight_scores, attention_scores, dependency_scores = get_semantic_weights(sentence, matcher)\n",
    "        final_score = {}\n",
    "        for k in sem_weight_scores:\n",
    "            try:\n",
    "                final_score[k] = final_score[k] = w1*sem_weight_scores[k] + w2*attention_scores[k] + w3*dependency_scores[k]\n",
    "            except Exception as e:\n",
    "                # print(e)\n",
    "                final_score[k] = sem_weight_scores[k]\n",
    "        masked_words = [k for k, v in sorted(final_score.items(), key=lambda item: item[1], reverse=True)]\n",
    "        final_masked_words = masked_words[:math.ceil(len(masked_words)*n)]\n",
    "        return mask_function(sentence, final_masked_words), masked_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Vsqfl78WtTzY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsqfl78WtTzY",
    "outputId": "820022cb-e939-46e5-f4ed-6824e55d43be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('mask', 17, 7)], ['fishies', 'food'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_masked_template(\"i am fishing for fishies because i love food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "IUKcRjsZul77",
   "metadata": {
    "id": "IUKcRjsZul77"
   },
   "outputs": [],
   "source": [
    "def apply_mask(sentence):\n",
    "    \"\"\"\n",
    "    Masks the specified spans in the sentence.\n",
    "\n",
    "    Parameters:\n",
    "    - sentence (str): The input sentence.\n",
    "    - mask_spans (list): List of 3-tuples specifying spans to mask.\n",
    "\n",
    "    Returns:\n",
    "    - str: Sentence with specified spans replaced by [MASK].\n",
    "    \"\"\"\n",
    "\n",
    "    # Reverse the list so that we can mask from the end of the sentence.\n",
    "    # This ensures that the earlier offsets don't change.\n",
    "    mask_spans,_ = get_masked_template(sentence)\n",
    "    mask_spans.reverse()\n",
    "\n",
    "    for _, offset, length in mask_spans:\n",
    "        sentence = sentence[:offset] + '[MASK]' + sentence[offset + length:]\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57338e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is [MASK]'s favorite [MASK]? quack \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_mask(\"What is drake's favorite drug? quack \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cce280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('jokegen/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "034c76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68d2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f.readlines()\n",
    "data = [i.strip() for i in data if len(i)<80]\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26c54304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12783"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f4034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82479743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 200/200 [00:28<00:00,  6.97it/s]\n"
     ]
    }
   ],
   "source": [
    "masked_data = []\n",
    "random.shuffle(data)\n",
    "for i in tqdm(data[:200]):\n",
    "    masked_data.append((i, apply_mask(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f579147a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"masked_templates_test.pkl\",\"wb\") as f:\n",
    "    pickle.dump(masked_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e964824",
   "metadata": {},
   "source": [
    "## Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4b87ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import AutoTokenizer, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03f28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filling_model = DistilBertForMaskedLM.from_pretrained('distilbert_model/')\n",
    "filling_tokenizer = DistilBertTokenizer.from_pretrained('distilbert_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0939313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_filler = pipeline(\"fill-mask\", model=model_w, tokenizer=tokenizer_w)\n",
    "mask_filler = pipeline(\"fill-mask\", model=filling_model, tokenizer=filling_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330a0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# mask_filler = pipeline(task=\"fill-mask\", model=\"bert_finetuning/final_model/\")\n",
    "\n",
    "def fill_in_the_blanks(sent, model, tokenizer, mask_filler):\n",
    "  # print(sent)\n",
    "  sent = sent.replace(\"[MASK]\",f\"{tokenizer.mask_token}\")\n",
    "  c = sent.count(f\"{tokenizer.mask_token}\")\n",
    "  for i in range(c):\n",
    "    s_embedding = model_s.encode(sent, convert_to_tensor=True)\n",
    "    sent = mask_filler(sent)\n",
    "    min_cosine_score = 1\n",
    "    for mask_candidates in sent:\n",
    "      if type(mask_candidates) is list:\n",
    "        for replacement in mask_candidates:\n",
    "          new_sent = replacement['sequence']\n",
    "          n_embedding = model_s.encode(new_sent, convert_to_tensor=True)\n",
    "          cosine_score = util.cos_sim(s_embedding, n_embedding)\n",
    "          if cosine_score < min_cosine_score:\n",
    "            min_cosine_score = cosine_score\n",
    "            best_candidate = new_sent\n",
    "      else:\n",
    "        new_sent = mask_candidates['sequence']\n",
    "        n_embedding = model_s.encode(new_sent, convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(s_embedding, n_embedding)\n",
    "        if cosine_score < min_cosine_score:\n",
    "          min_cosine_score = cosine_score\n",
    "          best_candidate = new_sent\n",
    "    sent = best_candidate\n",
    "  # print(sent)\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8deb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"masked_templates_test.pkl\",\"rb\") as f:\n",
    "    masked_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27273890",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27ce398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 1\n",
    "for i,j in masked_data:\n",
    "    generated = fill_in_the_blanks(j, model_w, tokenizer_w, mask_filler)\n",
    "    bert_outputs.append({\"original\": i, \"masked\": j, \"bert\":generated})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "937e9372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': \"What's Louis C.K.'s favorite type of meat other than his own? Jerkey\",\n",
       "  'masked': \"What's Louis C.K.'s [MASK] [MASK] of [MASK] other than his own? [MASK]\",\n",
       "  'bert': \"What's Louis C. K.'s favourite kind of music other than his own?!\"},\n",
       " {'original': 'Did you hear about the failed Origami shop? If folded.',\n",
       "  'masked': 'Did you hear about the failed Origami [MASK]? If folded.',\n",
       "  'bert': 'Did you hear about the failed Origami team? If folded.'},\n",
       " {'original': 'How much does a dead elephant weigh? A skele**ton**.',\n",
       "  'masked': 'How much does a [MASK] [MASK] weigh? A skele**ton**.',\n",
       "  'bert': 'How much does a capital whale weigh? A skele * * ton * *.'},\n",
       " {'original': 'How did the hipster burn his tongue? He ate his pizza BEFORE it was cool.',\n",
       "  'masked': 'How did the [MASK] burn his tongue? He ate his pizza BEFORE it was [MASK].',\n",
       "  'bert': 'How did the vampire burn his tongue? He ate his pizza BEFORE it was terrible.'},\n",
       " {'original': \"What do you call a basement full of SJW's? A whine cellar.\",\n",
       "  'masked': \"What do you call a [MASK] full of [MASK]'s? A [MASK] cellar.\",\n",
       "  'bert': \"What do you call a barn full of women's? A wine cellar.\"}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39cc4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bert_outputs.pkl\",\"wb\") as f:\n",
    "    pickle.dump(bert_outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6107cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = fill_in_the_blanks(\"Why did the [MASK] cross the [MASK]? To get to [MASK].\", filling_model, filling_tokenizer, mask_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4975eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac99936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_words_in_sentence(sentence, masked_template):\n",
    "    \"\"\"\n",
    "    Mask words in a sentence based on specified indices and offsets.\n",
    "\n",
    "    Parameters:\n",
    "    sentence (str): The input sentence.\n",
    "    masked_template (list): A list of tuples, each containing the word 'mask',\n",
    "                            a starting index, and an offset amount.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of masked words extracted from the original sentence.\n",
    "    \"\"\"\n",
    "    masked_words = []\n",
    "\n",
    "    # We'll work with the sentence as a mutable list of characters for easier processing.\n",
    "    sentence_chars = list(sentence)\n",
    "\n",
    "    # Processing each mask operation in the template.\n",
    "    for mask_operation in masked_template:\n",
    "        # Each operation should be a tuple: ('mask', start_index, offset)\n",
    "        if len(mask_operation) == 3 and mask_operation[0] == 'mask':\n",
    "            start_index = mask_operation[1]\n",
    "            offset = mask_operation[2]\n",
    "\n",
    "            # Bound checking to avoid index errors.\n",
    "            if start_index >= 0 and start_index + offset <= len(sentence_chars):\n",
    "                # Extract the word to be masked based on the start index and offset.\n",
    "                word_to_mask = sentence_chars[start_index : start_index + offset]\n",
    "\n",
    "                # Convert the characters back to a string.\n",
    "                word_to_mask_str = ''.join(word_to_mask)\n",
    "\n",
    "                # Add the extracted word to the list of masked words.\n",
    "                masked_words.append(word_to_mask_str)\n",
    "\n",
    "                # Mask the word in the original sentence characters list with a placeholder.\n",
    "                for i in range(start_index, start_index + offset):\n",
    "                    sentence_chars[i] = '*'  # Using '*' to denote masked characters.\n",
    "\n",
    "    return masked_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"masking_sample_set.pkl\",\"rb\") as f:\n",
    "    samples = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt4-selected-words.pkl\",\"rb\") as f:\n",
    "    ground = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37626d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2, w3 = 1,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290164e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_outputs, candidate_list = [], []\n",
    "for i in samples:\n",
    "    temp,candidates = get_masked_template(i,w1=w1, w2=w2, w3=w3)\n",
    "    model_out = mask_words_in_sentence(i,temp)\n",
    "    model_outputs.append(sorted(model_out))\n",
    "    candidate_list.append(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_data, err_data = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6640822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, ground_t, candidates, model_output in zip(samples, ground, candidate_list, model_outputs):\n",
    "    gr_set, ca_set = set(ground_t), set(candidates)\n",
    "    if gr_set.issubset(ca_set):\n",
    "        fin_data.append((sent, ground_t, candidates, model_output))\n",
    "    else:\n",
    "        err_data.append((sent, ground_t, candidates, model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(err_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(actual_keywords, predicted_keywords):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score based on lists of actual and predicted keywords.\n",
    "    \n",
    "    Parameters:\n",
    "    actual_keywords (list of str): The list of actual keywords.\n",
    "    predicted_keywords (list of str): The list of keywords predicted by the model.\n",
    "\n",
    "    Returns:\n",
    "    float: The F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert lists to sets to eliminate duplicates and allow for set operations\n",
    "    actual_set = set(actual_keywords)\n",
    "    predicted_set = set(predicted_keywords)\n",
    "\n",
    "    # Calculate true positive, false positive, and false negative\n",
    "    true_positive = len(actual_set.intersection(predicted_set))\n",
    "    false_positive = len(predicted_set.difference(actual_set))\n",
    "    false_negative = len(actual_set.difference(predicted_set))\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    if precision + recall == 0:  # Handle the case where both precision and recall are zero\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for s,g,c,m in fin_data:\n",
    "    if len(g)!=len(m):\n",
    "        continue\n",
    "    t.append(calculate_f1_score(g,m))\n",
    "p = [i[0] for i in t]\n",
    "r = [i[1] for i in t]\n",
    "f = [i[2] for i in t]\n",
    "n = len(p)\n",
    "x,y,z = sum(p)/n, sum(r)/n, sum(f)/n\n",
    "print(x,y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04df51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_els = []\n",
    "for s,g,c,m in fin_data:\n",
    "    words = []\n",
    "    for x in g:\n",
    "        words.append(x)\n",
    "    for x in range(len(c)-len(g)):\n",
    "        words.append('0')\n",
    "    random_els.append(random.sample(words, k=len(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904240d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for val,r in zip(fin_data, random_els):\n",
    "    s,g,c,m = val\n",
    "    if len(g)!=len(r):\n",
    "        print(g,r)\n",
    "        continue\n",
    "    t.append(calculate_f1_score(g,r))\n",
    "p = [i[0] for i in t]\n",
    "r = [i[1] for i in t]\n",
    "f = [i[2] for i in t]\n",
    "n = len(p)\n",
    "x,y,z = sum(p)/n, sum(r)/n, sum(f)/n\n",
    "print(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87fe2c",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## 1.\n",
    "\n",
    "0.33 0.33 0.33\n",
    "\n",
    "0.6395476190476188 0.6080476190476188 0.6209790764790761\n",
    "\n",
    "## 2.\n",
    "\n",
    "0.5 0.5 0\n",
    "\n",
    "0.6448809523809522 0.6128809523809522 0.6260822510822508\n",
    "\n",
    "## 3\n",
    "\n",
    "3 2 1\n",
    "\n",
    "0.6498809523809522 0.6192142857142856 0.6317886002886"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01edc646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "28cc239a30b7494aa2b7fb667527caab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3af998f994924adcb46664dd35b58ce8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6944befc8fd34698b85f6c773a2c55a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77c13019fe4d41d4a9a452dd0bf62ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81a49e890316453f8b72743e38be4129": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "854bd183f6f2494494c670d071aeb959": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3e4c8f3e53c4eb98a800f3c36e78c0c",
       "IPY_MODEL_f4d2d10d516845d5bc63e23cd0d6f099",
       "IPY_MODEL_d9a318b1a95d4e86b9272c0b2bdcebbc"
      ],
      "layout": "IPY_MODEL_6944befc8fd34698b85f6c773a2c55a1"
     }
    },
    "aed91f6e635b4c29a6099b9914741897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e4c8f3e53c4eb98a800f3c36e78c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aed91f6e635b4c29a6099b9914741897",
      "placeholder": "​",
      "style": "IPY_MODEL_81a49e890316453f8b72743e38be4129",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "c29bb58d4aba44bba7e76089d7fee812": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9a318b1a95d4e86b9272c0b2bdcebbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c29bb58d4aba44bba7e76089d7fee812",
      "placeholder": "​",
      "style": "IPY_MODEL_77c13019fe4d41d4a9a452dd0bf62ab5",
      "value": " 466k/466k [00:00&lt;00:00, 840kB/s]"
     }
    },
    "f4d2d10d516845d5bc63e23cd0d6f099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28cc239a30b7494aa2b7fb667527caab",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3af998f994924adcb46664dd35b58ce8",
      "value": 466062
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
